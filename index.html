<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  
  <title>Yet Another S4 Blog</title>
  <style>
    :root {
      --title-color: #bf5700; /* Change this to whatever color you want */
    }

    body {
      margin: 0;
      padding: 20px;
      font-family: Arial, sans-serif;
    }

    header h1 {
      color: var(--title-color);
      font-family: var(--title-font: Arial, sans-serif);
      text-align: center;
      font-size: 2.5em;
      margin-top: 20px;
    }

    article {
      max-width: 700px;
      margin: 40px auto;
      line-height: 1.6;
      color: #333;
    }

    a {
    color: #00A9B7; /* Set your desired color */
    text-decoration: underline; /* Optional: removes underline */
    }

    h2 {
      font-size: 1.8em;
    }

    /* Ensure math equations are left-aligned */
    .math-center {
      text-align: left; /* Left-align the entire block */
      display: block;   /* Ensure it's treated as a block-level element */
      margin: 30px 0;
    }

    /* Additional specific targeting for MathJax rendered equations */
    .mjx-chtml {
      text-align: left !important; /* Force MathJax to align content left */
    }

    .mjx-math {
      text-align: left !important; /* Ensure MathJax equations are left-aligned */
    }
  </style>
</head>
<body>
  <header>
    <h1>Yet Another S4 Blog</h1>
  </header>

  <article>
    <h2>Introduction</h2>
    <p>The aim of this blog is to concisely aid those who wish to learn the basic mathematical background of 
    and to learn how to implement the Structured State Space for Sequence Modeling (S4) architecture. This will be done
    in two sections: (1) mathematical background and (2) code implementation in Python and Pytorch. The target audience
    of this blog are those who are new to the S4 architecture and wish to learn how to implement it for research or other purposes.
    Though other tutorials and blogs exist (like <a href="https://srush.github.io/annotated-s4/" target="_blank">The Annotated S4</a>
    by A. Gu, K. Goel, and C. RÃ©) this blog will try to distinguish itself from the rest by making each step of the explanation as explicit as possible to 
    make the technical content more readily digestible. </p>
  </article>

  <article>
    <h2>Mathematical Background</h2>
    <p>The S4 architecture takes inspiration from the linear time invariant (LTI) dynamical system, where a hidden state is updated using
    data inputs, which is then used to evolve the system with linear operators.</p>

    <div class="math-center">
      \[
      \begin{aligned}
        h_{k+1} &= Ah_k + Bx_k \\
        \hat{x}_{k+1} &= Ch_{k+1} + Dx_k
      \end{aligned}
      \]
    </div>

    <p>For a time-series prediction task, where a sequence of ground-truth state variables (\(x_n\))
    is used to predict the next step (\(\hat{x}_{n+1}\)) - ie. given ground truth variables from timestep 0 to n (\([x_0, x_1, ..., x_n]\)),
    predict the value of the state variable at the next timestep (\(\hat{x}_{n+1}\)). The dimensionality of the ground truth and predicted state
    variables and the hidden state (h) are depicted below:</p> 
    <div class="math-center">
      \[
      \begin{aligned}
        &x_j, \hat{x}_j \in \mathbb{R}^{d} \\
        &h \in \mathbb{R}^{\symcal{h}}
      \end{aligned}
      \]
    </div>
    <p>Setting the hidden state at timestep = 0 shows</p>
    <div class="math-center">
      \[
      \begin{aligned}
        h_1 &= Ah_0 + Bx_0 = Bx_0\\
        \hat{x}_1 &= Ch_1 + Dx_0 = CBx_0 + Dx_0\\
        &\\
        h_2 &= Ah_1 + Bx_1 = ABx_0 + Bx_1\\
        \hat{x}_2 &= Ch_2 + Dx_1 = CABx_0 + CBx_1 + Dx_1\\
        &\\
        h_3 &= Ah_2 + Bx_2 = A^2Bx_0 + ABx_1 + Bx_2\\
        \hat{x}_3 &= Ch_3 + Dx_2 = CA^2Bx_0  +CABx_1 + CBx_2 + Dx_2\\
        &\\
        h_4 &= Ah_3 + Bx_3 = A^3Bx_0 + A^2Bx_1 + ABx_2 + Bx_3\\
        \hat{x}_4 &= Ch_4 + Dx_3 = CA^3Bx_0 + CA^2Bx_1  +CABx_2 + CBx_3 + Dx_3\\
        &\\
      &\text{and so on...}
      \end{aligned}
      \]
    </div>
    <p>From this explicit iteration, it can be shown that the predicted output (\(\hat{x}_{n+1}\)) follows 
    a formula:</p>
    <div class="math-center">
      \[
      \hat{x}_{n+1} = \sum_{j=0}^{n}CA^{n-j}Bx_j + Dx_n
      \]
    </div>
    <p>For various purposes, let A be the sum of a normal matrix (a matrix that can be expressed as 
      \(V\Lambda V^*\) where V is a unitary matrix, \(VV^*=V^*V=I\), and \(\Lambda\) is a diagonal matrix) and
      a low rank matrix (an outer product of two low rank matrices): \(A = V\Lambda V^*-PQ^*\), ie. a normal plus
      low-rank (NPLR) matrix. Upon a unitary transformation (multiplication by a unitary matrix and its conjugate transpose on either side), 
      this NPLR matrix can be converted to a diagonal plus low-rank (DPLR) matrix. This is done because further mathematical 
      simplifications can be done when the A matrix is DPLR.</p>

    <div class="math-center">
      \[
      \begin{aligned}
        A &= V\Lambda V^* - PQ^*\\
        V^*AV &= V^*(V\Lambda V^* - PQ^*)V=\Lambda - V^*PQ^*V\\
        &=\Lambda - \hat{P}\hat{Q}^*
      \end{aligned}
      \]
    </div>
    Where \(\hat{P} = V^*P\) and \( \hat{Q} = V^*Q\) (as a refresher, conjugate transpose rules 
    for the product of two matricies are as follows: \((AB)^*=B^*A^*\)).

    <p>Dimensions for the parameters are as follows:</p>
    <div class="math-center">
      \[
      \begin{aligned}
        &\Lambda \in \mathbb{C}^{h\times h}\text{ (diagonal matrix)} \\
        &P, Q \in \mathbb{C}^{h \times r} \\
        &B, C \in \mathbb{C}^{h \times d}
      \end{aligned}
      \]
    </div>

    <p> Although many discretization methods exist, the Tustin discretization will be utilized for demonstration purposes. See below
    for the pseudo code for obtaining the convolution kernels (\(\bar{K}\)). </p>
    <div class="math-center">
      \[
      \begin{aligned}
        &\hat{A} = \Lambda - \hat{P}\hat{Q}^* \in \mathbb{C}^{h\times h} \\
        &\bar{A} = \left( I_{h\times h} - \frac{\Delta}{2}\hat{A}\right)^{\dagger}\left( I + \frac{\Delta}{2}\hat{A}\right) \in \mathbb{C}^{h\times h} \\
        &\tilde{C} = \left( I_{h\times h} - \bar{A}^\ell \right)^*\hat{C} \in \mathbb{C}^{h\times d} \\
        &\text{for w = 0, 1, 2, ..., \(\ell\) - 1:}\\
        &\;\;\;\;z[w] = \text{exp}\left(-2\pi i \frac{w}{\ell}\right) \in \mathbb{C},\;\;\left(z\in\mathbb{C}^{\ell \times 1}\right) \\
        &\;\;\;\;R[w] = \left( \frac{2}{\Delta}\frac{1-z[w]}{1+z[w]}I_{h\times h}-\Lambda \right)^{-1} \in \mathbb{C}^{h\times h}
          ,\;\;\left(R\in\mathbb{C}^{\ell \times h \times h}\right)  \\
        &\;\;\;\;\text{if r = 1}:\\
        &\;\;\;\;\;\;\;\;\mathcal{K}[w] = \frac{2}{1+z[w]}\tilde{C}^*R[w]\left(I_{h\times h}-\frac{1}{1+\hat{Q}^*R[w]\hat{P}}\hat{P}\hat{Q}^*R[w]\right)\hat{B}\in \mathbb{C}^{d\times d},\;\;\left(\mathcal{K}\in\mathbb{C}^{\ell\times d\times d}\right) \\
        &\;\;\;\;\text{else if r > 1}:\\
        &\;\;\;\;\;\;\;\;\mathcal{K}[w] = \frac{2}{1+z[w]}\tilde{C}^*R[w]\left[I_{h\times h} - P\left(I_{r\times r}+\hat{Q}^*R[w]P\right)^{\dagger}\hat{Q}^*R[w]\right]\hat{B} 
          \in \mathbb{C}^{d\times d},\;\;\left(\mathcal{K}\in\mathbb{C}^{\ell\times d\times d}\right) \\
        &\bar{K} = \text{real}\left(\text{iFFT}\left(\mathcal{K},\text{ axis = 0}\right)\right) \in \mathbb{R}^{\ell\times d \times d}
      \end{aligned} 
      \]
    </div>
  <p>\underline{Couple notes:}</p>
  <ol>
  <li>\((\cdot)[w]\) is the w'th quantity of \((\cdot)\)</li>
  <li>\((\cdot)^\dagger\) is the Moore-Penrose pseudoinverse of \((\cdot)\).
  The pseudoinverse is used when inverting non-diagonal matrices for stability, and identity matrix dimensions are shown to avoid confusion.</li>
  </ol>
  <p>\underline{Couple notes:} \((\cdot)[w]\) is the w'th quantity of \((\cdot)\), \((\cdot)^\dagger\) is the Moore-Penrose pseudoinverse of \((\cdot)\).
  The pseudoinverse is used when inverting non-diagonal matrices for stability, and identity matrix dimensions are shown to avoid confusion.</p>
  </article>

  <article>
    <h2>Code Implementation</h2>
    <p>Now that the math and logic have been established, a code implementation walkthrough will incorporate the above content
    to make a working S4 model.</p>
  </article>
</body>

<article>
    <h2> About the Author</h2>
    <p> 
      My name is Austin Kwon. I am a Computational Science, Engineering, and Mathematics PhD student at 
      the <a href = "https://oden.utexas.edu/" target = "_black">Oden Institute</a> at the University of Texas at Austin. 
      I received my BS in Aerospace and Mechanical Engineering at Rensselaer Polytechnic Institute in 2024.
      Any questions can be directed to austin.kwon@utexas.edu.
    </p>
  </article>

<div style="text-align: center;">
  <img src="oden_logo.png" alt="ODEN Logo" width="210">
</div>
</html>
